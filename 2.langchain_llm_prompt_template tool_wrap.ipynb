{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0893ac-bc1a-410c-b8b6-5e6a743e2aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Question: What is LangGraph in LangChain?\n",
      "Tool Answer: In LangChain, LangGraph refers to a graph data structure that represents the relationships between different entities and pieces of information within a language model's output. It's essentially a way to visualize and interact with the knowledge graph generated by the model.\n",
      "\n",
      "Think of it like a map of concepts, entities, and their connections, which can be used for various tasks such as:\n",
      "\n",
      "1. **Knowledge graph construction**: LangGraph helps build a structured representation of the knowledge extracted from the language model.\n",
      "2. **Question answering**: It enables more accurate question-answering by allowing models to reason over the relationships between entities in the graph.\n",
      "3. **Text generation**: LangGraph can be used as a source of information for generating text, such as summaries or explanations.\n",
      "\n",
      "In summary, LangGraph is an essential component of LangChain that facilitates the creation and manipulation of knowledge graphs from language model outputs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# üî∏ Step 1: Initialize LOCAL LLM (Ollama)\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.1\",   # or \"llama2\"\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# üî∏ Step 2: Create Prompt Template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer clearly: {question}\"\n",
    ")\n",
    "\n",
    "# üî∏ Step 3: Create Runnable chain (NEW STYLE)\n",
    "chain = prompt | llm\n",
    "\n",
    "# üî∏ Step 4: Wrap chain as a Tool\n",
    "qa_tool = Tool(\n",
    "    name=\"Simple QA Tool\",\n",
    "    func=lambda q: chain.invoke({\"question\": q}),\n",
    "    description=\"Answers clear questions using a local Ollama LLM\"\n",
    ")\n",
    "\n",
    "# üî∏ Step 5: Use the Tool\n",
    "query = \"What is LangGraph in LangChain?\"\n",
    "answer = qa_tool.run(query)\n",
    "\n",
    "# üñ®Ô∏è Output\n",
    "print(\"User Question:\", query)\n",
    "print(\"Tool Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada560d5-15aa-4d49-bde6-ec5ed9e3ce85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchainhope)",
   "language": "python",
   "name": "langchainhope"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
