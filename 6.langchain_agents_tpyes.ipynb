{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105af63c-7c75-4590-b23f-1fe58cfa83c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßë‚Äçüíª Tool: QA\n",
      "Input: What is LangGraph in LangChain?\n",
      "\n",
      "ü§ñ Output:\n",
      "LangChain is an open-source library for building conversational AI applications. LangGraph is a component of LangChain that enables the creation and manipulation of graph-like data structures, specifically designed to represent complex relationships between entities and concepts.\n",
      "\n",
      "In essence, LangGraph allows developers to build and query knowledge graphs, which are graphical representations of entities, their attributes, and the relationships between them. This feature is particularly useful for applications like question answering, entity disambiguation, and conversational dialogue management.\n",
      "\n",
      "LangGraph provides a flexible and scalable way to represent and reason about complex knowledge graphs, making it an essential tool for building sophisticated conversational AI systems with LangChain.\n",
      "\n",
      "üßë‚Äçüíª Tool: Web Search\n",
      "Input: Latest updates on GPT-4\n",
      "\n",
      "ü§ñ Output:\n",
      "\n",
      "\n",
      "üßë‚Äçüíª Tool: QA\n",
      "Input: Explain LangChain memory in brief\n",
      "\n",
      "ü§ñ Output:\n",
      "**LangChain Memory**\n",
      "\n",
      "In LangChain, the memory component is designed to store and manage context information across multiple turns of a conversation. It allows developers to maintain a persistent state that can be accessed and updated throughout the conversation.\n",
      "\n",
      "Think of it like a digital \"notebook\" where you can save relevant information, such as user preferences, previous conversations, or intermediate results from computations. This enables your conversational AI model to recall and build upon previously discussed topics, making the conversation more coherent and engaging.\n",
      "\n",
      "LangChain memory provides a flexible way to store and retrieve context-dependent data, which is essential for building conversational AI applications that require retaining information across multiple turns.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.tools import Tool\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# =========================\n",
    "# Simple conversation memory\n",
    "# =========================\n",
    "class SimpleMemory:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def add(self, role, message):\n",
    "        self.history.append(f\"{role}: {message}\")\n",
    "\n",
    "    def get_context(self):\n",
    "        # Join past messages as a string context\n",
    "        return \"\\n\".join(self.history)\n",
    "\n",
    "memory = SimpleMemory()\n",
    "\n",
    "# =========================\n",
    "# Initialize Ollama LLM\n",
    "# =========================\n",
    "llm = OllamaLLM(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "# =========================\n",
    "# Tool 1: QA Tool\n",
    "def qa_tool_func(question):\n",
    "    context = memory.get_context()\n",
    "    prompt_text = f\"{context}\\nUser Question: {question}\\nAnswer clearly:\"\n",
    "    response = llm.invoke(prompt_text)  # CRT fix: pass string, not dict\n",
    "    memory.add(\"User\", question)\n",
    "    memory.add(\"AI\", response)\n",
    "    return response\n",
    "\n",
    "qa_tool = Tool(\n",
    "    name=\"QA Tool\",\n",
    "    func=qa_tool_func,\n",
    "    description=\"Answer factual questions clearly using memory\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Tool 2: Web Search Tool\n",
    "def web_search_tool(query):\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    results = []\n",
    "    for g in soup.find_all('div', class_='tF2Cxc')[:3]:\n",
    "        title = g.find('h3').text if g.find('h3') else \"\"\n",
    "        link = g.find('a')['href'] if g.find('a') else \"\"\n",
    "        results.append(f\"{title} - {link}\")\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"Web Search\",\n",
    "    func=web_search_tool,\n",
    "    description=\"Search Google without API key\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Tools dictionary\n",
    "# =========================\n",
    "tools = {\n",
    "    \"QA\": qa_tool,\n",
    "    \"Web Search\": search_tool\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Run queries manually\n",
    "# =========================\n",
    "queries = [\n",
    "    (\"QA\", \"What is LangGraph in LangChain?\"),\n",
    "    (\"Web Search\", \"Latest updates on GPT-4\"),\n",
    "    (\"QA\", \"Explain LangChain memory in brief\")\n",
    "]\n",
    "\n",
    "for tool_name, user_input in queries:\n",
    "    print(f\"\\nüßë‚Äçüíª Tool: {tool_name}\")\n",
    "    print(f\"Input: {user_input}\")\n",
    "    output = tools[tool_name].run(user_input)\n",
    "    print(f\"\\nü§ñ Output:\\n{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f78c65a-89d9-4c84-8fe4-a35700f1c14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Summarize LangChain in 2 lines, then tell me who created it.\n",
      "Response: **Summary of LangChain:**\n",
      "\n",
      "LangChain is an open-source library for building conversational AI applications. It provides a set of components, including LangGraph and memory, to enable the creation of sophisticated conversational AI systems.\n",
      "\n",
      "**Creator of LangChain:**\n",
      "LangChain was created by Meta AI.\n"
     ]
    }
   ],
   "source": [
    "query = \"Summarize LangChain in 2 lines, then tell me who created it.\"\n",
    "print(\"Query:\", query)\n",
    "print(\"Response:\", qa_tool.run(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b4bda-d9c8-4e93-b8f5-3e44cf41cba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412578b9-2b62-4b77-acac-b3f56c9b32a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9aa124-ab86-4c35-a034-25b6a3d8cde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb71380-eaf1-4309-8694-9abdd344edb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5db5ac-5245-41b1-bbd6-5db72e53b3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161f827-1a7d-4ef4-9c85-6c6988bcb99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb41338-7f5f-41f6-82cc-5bd84d9d20a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04733c8-9bc8-48f2-90db-8ef01a847566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchainhope)",
   "language": "python",
   "name": "langchainhope"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
