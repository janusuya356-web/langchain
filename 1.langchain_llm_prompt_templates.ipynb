{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f260e3-e4cc-4759-a1f4-18bc984b5091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Question: What is LangGraph in LangChain?\n",
      "LLM Answer: In LangChain, LangGraph refers to a graph data structure that represents the relationships between different entities and pieces of information within a language model's output. It's essentially a way to visualize and interact with the knowledge graph generated by the model.\n",
      "\n",
      "Think of it like a map of concepts, entities, and their connections, which can be used for various tasks such as:\n",
      "\n",
      "1. **Knowledge graph construction**: LangGraph helps build a structured representation of the knowledge extracted from the language model.\n",
      "2. **Question answering**: It enables more accurate question-answering by allowing models to reason over the relationships between entities in the graph.\n",
      "3. **Text generation**: LangGraph can be used as a source of information for generating text, such as summaries or explanations.\n",
      "\n",
      "In summary, LangGraph is an essential component of LangChain that facilitates the creation and manipulation of knowledge graphs from language model outputs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# üî∏ Initialize Ollama LLM (LOCAL)\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.1\",   # or llama2\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# üî∏ Prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer clearly: {question}\"\n",
    ")\n",
    "\n",
    "# üî∏ NEW STYLE: Runnable chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# üöÄ Run query\n",
    "query = \"What is LangGraph in LangChain?\"\n",
    "response = chain.invoke({\"question\": query})\n",
    "\n",
    "# üñ®Ô∏è Output\n",
    "print(\"User Question:\", query)\n",
    "print(\"LLM Answer:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f72bc3-49d6-487d-80c2-5cf681c2cac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f99369-4c3f-4077-be15-4516dcc41483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b4d1b-f6c5-43d0-bd62-18a3a279cd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchainhope)",
   "language": "python",
   "name": "langchainhope"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
